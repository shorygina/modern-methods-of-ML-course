{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/t1webrwixdlaacv/space_ga.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с батч-оптимизацией(2 балла)\n",
    "\n",
    "Рассмотрим случай, когда данных в выборке много. В таких случаях используется стохастическая или батч-оптимизация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные из файла space_ga.csv и нормализуйте их. Мы будем предсказывать первый столбец по шести остальным. Эти данные получены с выборов в США в 1980 году."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы могли заметить, датасет больше предыдущего. На нём мы попробуем батч-оптимизацию.\n",
    "\n",
    "Измените функцию для минимизации написанную на семинаре так, чтобы на вход они принимала дополнительный параметр — размер батча. Запустите функцию при разных размерах батча. Прокомментируйте результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двумерная классификация(1 балл)\n",
    "\n",
    "Решим задачу 2D классификации синтетических данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('train.npy', 'rb') as fin:\n",
    "    X = np.load(fin)\n",
    "    \n",
    "with open('target.npy', 'rb') as fin:\n",
    "    y = np.load(fin)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Как можно заметить, данные сверху линейно неразделимы. Поэтому мы должны добавить дополнительные признаки(или использовать нелинейную модель). Можно заметить, что гиперплоскость разделяющая два класса принимает форму круга, поэтому мы можем добавить квадратичные признаки чтобы сделать классы линейно разделимыми.\n",
    "\n",
    "\n",
    "![](kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand(X):\n",
    "    \"\"\"\n",
    "    Adds quadratic features. \n",
    "    This expansion allows your linear model to make non-linear separation.\n",
    "    \n",
    "    For each sample (row in matrix), compute an expanded row:\n",
    "    [feature0, feature1, feature0^2, feature1^2, feature0*feature1, 1]\n",
    "    \n",
    "    :param X: matrix of features, shape [n_samples,2]\n",
    "    :returns: expanded features of shape [n_samples,6]\n",
    "    \"\"\"\n",
    "    X_expanded = np.zeros((X.shape[0], 6))\n",
    "    \n",
    "    # TODO:<your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_expanded = expand(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple test on random numbers\n",
    "\n",
    "dummy_X = np.array([\n",
    "        [0,0],\n",
    "        [1,0],\n",
    "        [2.61,-1.28],\n",
    "        [-0.59,2.1]\n",
    "    ])\n",
    "\n",
    "# call your expand function\n",
    "dummy_expanded = expand(dummy_X)\n",
    "\n",
    "# what it should have returned:   x0       x1       x0^2     x1^2     x0*x1    1\n",
    "dummy_expanded_ans = np.array([[ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  1.    ],\n",
    "                               [ 1.    ,  0.    ,  1.    ,  0.    ,  0.    ,  1.    ],\n",
    "                               [ 2.61  , -1.28  ,  6.8121,  1.6384, -3.3408,  1.    ],\n",
    "                               [-0.59  ,  2.1   ,  0.3481,  4.41  , -1.239 ,  1.    ]])\n",
    "\n",
    "#tests\n",
    "assert isinstance(dummy_expanded,np.ndarray), \"please make sure you return numpy array\"\n",
    "assert dummy_expanded.shape == dummy_expanded_ans.shape, \"please make sure your shape is correct\"\n",
    "assert np.allclose(dummy_expanded,dummy_expanded_ans,1e-3), \"Something's out of order with features\"\n",
    "\n",
    "print(\"Seems legit!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия(3 балла)\n",
    "\n",
    "Для классификации объектов мы будем получать вероятность того что объект принадлежит к классу '1'. Чтобы предсказывать вероятность мы будем использовать вывод линейной модели и логистической функции:\n",
    "\n",
    "\n",
    "$$ a(x; w) = \\langle w, x \\rangle $$\n",
    "$$ P( y=1 \\; | \\; x, \\, w) = \\dfrac{1}{1 + \\exp(- \\langle w, x \\rangle)} = \\sigma(\\langle w, x \\rangle)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability(X, w):\n",
    "    \"\"\"\n",
    "    Given input features and weights\n",
    "    return predicted probabilities of y==1 given x, P(y=1|x), see description above\n",
    "        \n",
    "    Don't forget to use expand(X) function (where necessary) in this and subsequent functions.\n",
    "    \n",
    "    :param X: feature matrix X of shape [n_samples,6] (expanded)\n",
    "    :param w: weight vector w of shape [6] for each of the expanded features\n",
    "    :returns: an array of predicted probabilities in [0,1] interval.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO:<your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_weights = np.linspace(-1, 1, 6)\n",
    "ans_part1 = probability(X_expanded[:1, :], dummy_weights)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для логистической регрессии оптимальное значение весов $w$ находится с помощью минимизации кросс-энтропии:\n",
    "\n",
    "\n",
    "Loss для одного сэмпла: $$ l(x_i, y_i, w) = - \\left[ {y_i \\cdot log P(y_i = 1 \\, | \\, x_i,w) + (1-y_i) \\cdot log (1-P(y_i = 1\\, | \\, x_i,w))}\\right] $$\n",
    "\n",
    "Loss для нескольких сэмплов: $$ L(X, \\vec{y}, w) =  {1 \\over \\ell} \\sum_{i=1}^\\ell l(x_i, y_i, w) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,6], target vector [n_samples] of 1/0,\n",
    "    and weight vector w [6], compute scalar loss function L using formula above.\n",
    "    Keep in mind that our loss is averaged over all samples (rows) in X.\n",
    "    \"\"\"\n",
    "    # TODO:<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Т.к мы обучаем нашу модель с помощью градиентного спуска мы должны считать градиенты.\n",
    "Для этого нам нужны производные функции потерь по каждому из весов.\n",
    "\n",
    "\n",
    "$$ \\nabla_w L = {1 \\over \\ell} \\sum_{i=1}^\\ell \\nabla_w l(x_i, y_i, w) $$ \n",
    "\n",
    "Выведите формулу для подсчета градиента.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_grad(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,6], target vector [n_samples] of 1/0,\n",
    "    and weight vector w [6], compute vector [6] of derivatives of L over each weights.\n",
    "    Keep in mind that our loss is averaged over all samples (rows) in X.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательная функция для визуализации предсказаний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "def visualize(X, y, w, history):\n",
    "    \"\"\"draws classifier prediction with matplotlib magic\"\"\"\n",
    "    Z = probability(expand(np.c_[xx.ravel(), yy.ravel()]), w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history)\n",
    "    plt.grid()\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.ylim(0, ymax)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualize(X, y, dummy_weights, [0.5, 0.5, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "В данной секции мы будем использовать функции, написанные вами, чтобы обучить наш классификатор с помощью стохастического градиентного спуска.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch SGD(1 балл)\n",
    "\n",
    "Стохастический градиентный спуск берет рандомный батч из $m$ сэмплов на каждой итерации, подсчитывает градиент функции потерь на этом батче и делает шаг градиентного спуска:\n",
    "\n",
    "$$ w_t = w_{t-1} - \\eta \\dfrac{1}{m} \\sum_{j=1}^m \\nabla_w l(x_{i_j}, y_{i_j}, w_t) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([0, 0, 0, 0, 0, 1])\n",
    "\n",
    "eta= 0.1 # learning rate\n",
    "\n",
    "n_iter = 100\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X_expanded.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X_expanded, y, w)\n",
    "    if i % 10 == 0:\n",
    "        visualize(X_expanded[ind, :], y[ind], w, loss)\n",
    "\n",
    "    # Keep in mind that compute_grad already does averaging over batch for you!\n",
    "    # TODO:<your code here>\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with momentum(1 балл)\n",
    "\n",
    "Momentum это метод позволяющий корректировать шаг SGD в нужное направление и уменьшать осцилляции как показано на рисунке. Данный эффект достигается с помощью добавления предыдущих шагов с коэффициентом $\\alpha$ к текущему градиенту для каждого шага с обновлением весов.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ \\nu_t = \\alpha \\nu_{t-1} + \\eta\\dfrac{1}{m} \\sum_{j=1}^m \\nabla_w l(x_{i_j}, y_{i_j}, w_t) $$\n",
    "$$ w_t = w_{t-1} - \\nu_t$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "![](sgd.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([0, 0, 0, 0, 0, 1])\n",
    "\n",
    "eta = 0.05 # learning rate\n",
    "alpha = 0.9 # momentum\n",
    "nu = np.zeros_like(w)\n",
    "\n",
    "n_iter = 100\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X_expanded.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X_expanded, y, w)\n",
    "    if i % 10 == 0:\n",
    "        visualize(X_expanded[ind, :], y[ind], w, loss)\n",
    "\n",
    "    # TODO:<your code here>\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM(2 балла)\n",
    "Реализуйте метод ADAM, использующий градиенты и квадраты градиентов сглаженные экспоненциальным скользящим средним:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "m_t &=& \\beta_1 m_{t-1} + (1-\\beta_1) g_t\\\\\n",
    "s_t &=& \\beta_2 s_{t-1} + (1-\\beta_2) g_t^2 \\\\\n",
    "w_t &=& w_{t-1} - \\eta \\times \\frac{\\sqrt{ 1 -  \\beta_2^t}}{ 1 - \\beta_1^t} \\times \\frac{ m_t }{ \\sqrt{s_t+eps}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "w = np.array([0, 0, 0, 0, 0, 1.])\n",
    "\n",
    "eta = 0.1 # learning rate\n",
    "beta_1 = 0.9 # moving average of gradient\n",
    "beta_2 = 0.999 # moving average of gradient norm squared\n",
    "g2 = None # we start with None so that you can update this value correctly on the first iteration\n",
    "eps = 1e-8\n",
    "\n",
    "n_iter = 100\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X_expanded.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X_expanded, y, w)\n",
    "    if i % 10 == 0:\n",
    "        visualize(X_expanded[ind, :], y[ind], w, loss)\n",
    "\n",
    "    # TODO:<your code here>\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
